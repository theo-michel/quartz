<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="### Math Primers  [[maths_primer.pdf]]
#TODO page 6 of mi exercises perceptron
MI1 [[Notation MI1]]
1.2 Perceptron and feature extraction How to train a connectionnist neuron & function fitting?"><meta property="og:title" content><meta property="og:description" content="### Math Primers  [[maths_primer.pdf]]
#TODO page 6 of mi exercises perceptron
MI1 [[Notation MI1]]
1.2 Perceptron and feature extraction How to train a connectionnist neuron & function fitting?"><meta property="og:type" content="website"><meta property="og:image" content="https://theo-michel.github.io/quartz/icon.png"><meta property="og:url" content="https://theo-michel.github.io/quartz/Machine-intelligence-1/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="### Math Primers  [[maths_primer.pdf]]
#TODO page 6 of mi exercises perceptron
MI1 [[Notation MI1]]
1.2 Perceptron and feature extraction How to train a connectionnist neuron & function fitting?"><meta name=twitter:image content="https://theo-michel.github.io/quartz/icon.png"><meta name=twitter:site content="_jzhao"><title>ðŸª´ Quartz 3.3</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://theo-michel.github.io/quartz//icon.png><link href=https://theo-michel.github.io/quartz/styles.b369a84b3c6e6bfd686ad1f9da65641c.min.css rel=stylesheet><link href=https://theo-michel.github.io/quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://theo-michel.github.io/quartz/js/darkmode.c8f8d7ab42782baefb49de2020b64bb7.min.js></script>
<script src=https://theo-michel.github.io/quartz/js/util.a0ccf91e1937fe761a74da4946452710.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script async src=https://theo-michel.github.io/quartz/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://theo-michel.github.io/quartz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://theo-michel.github.io/quartz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://theo-michel.github.io/quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://theo-michel.github.io/quartz/",fetchData=Promise.all([fetch("https://theo-michel.github.io/quartz/indices/linkIndex.f3e917f6144886a88a6bb520e9d5493b.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://theo-michel.github.io/quartz/indices/contentIndex.9e300de122cb6bc5047d4f6325724d61.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://theo-michel.github.io/quartz",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://theo-michel.github.io/quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/theo-michel.github.io\/quartz\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=theo-michel.github.io/quartz src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://theo-michel.github.io/quartz/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://theo-michel.github.io/quartz/>ðŸª´ Quartz 3.3</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Unknown
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/Machine%20intelligence%201.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><ol><li><a href=#mi1>MI1</a></li></ol></li><li><a href=#12-perceptron-and-feature-extraction>1.2 Perceptron and feature extraction</a></li><li><a href=#13-multi-layer-perceptron>1.3 Multi-Layer Perceptron</a><ol><li><a href=#model>Model</a></li><li><a href=#performance-measure>Performance Measure</a></li><li><a href=#optimisation-of-model-parameters>Optimisation of model parameters</a></li><li><a href=#validation>Validation</a></li></ol></li><li><a href=#14-additionnal-topics>1.4 Additionnal topics</a><ol><li><a href=#stochastic-approximation-and-online-learning>Stochastic Approximation and Online Learning</a></li><li><a href=#improving-gradient-descent-optimization-batch>Improving Gradient Descent Optimization (Batch)</a></li><li><a href=#the-conjugate-gradient-method>The Conjugate Gradient Method</a></li><li><a href=#overfitting-and-underfitting>Overfitting and Underfitting</a></li><li><a href=#bias-and-variance>Bias and Variance</a></li><li><a href=#regularization>Regularization</a></li><li><a href=#classification-problems-multi-class>Classification Problems (Multi-Class)</a></li></ol></li><li><a href=#15-deep-learning>1.5 Deep Learning</a><ol><li><a href=#auto-encoders>[[Auto-Encoders]]</a></li><li><a href=#new-training-method>New training method</a></li><li><a href=#regularization-1>Regularization</a></li><li><a href=#convolutionnal-neural-network>[[Convolutionnal Neural Network]]</a></li></ol></li><li><a href=#161>1.6.1</a></li><li><a href=#162-recurent-neural-networks>1.6.2 Recurent Neural Networks</a></li><li><a href=#long-term-dependencie>Long-term dependencie</a><ol><li><a href=#other-recurent-architectures>Other recurent architectures</a></li></ol></li><li><a href=#17-rbf-networks>1.7 RBF networks</a></li></ol><ol><li><ol><li><a href=#for-linear-classification>For linear classification</a></li><li><a href=#formulation-of-the-inductive-learning-problem>Formulation of the inductive learning problem</a></li></ol></li><li><a href=#22-support-vector-machines>2.2 Support vector machines</a><ol><li><a href=#support-vector-regression>Support vector regression</a></li></ol></li><li><a href=#31-uncertainty-and-inference>3.1 Uncertainty and inference</a></li><li><a href=#32-bayesian-networks>3.2 Bayesian networks</a></li><li><a href=#graph-inference>Graph inference</a></li><li><a href=#33-bayesian-inference-and-neural-networks>3.3 Bayesian inference and neural networks</a></li></ol><ol><li><a href=#41-valuation>4.1 Valuation</a></li><li><a href=#42-improvment>4.2 Improvment</a></li><li><a href=#deep-learning>Deep Learning</a></li><li><a href=#conjugate-gradient-method>Conjugate gradient method</a></li><li><a href=#regularisation>Regularisation</a></li><li><a href=#validation-1>Validation</a></li><li><a href=#lstm>[[LSTM]]</a></li><li><a href=#class-imbalance>Class imbalance:</a></li><li><a href=#k-nearest-neighbours>![[K Nearest Neighbours]]</a></li><li><a href=#radial-basis-function>[[Radial Basis Function]]</a></li></ol><ol><li><a href=#questions>Questions</a></li></ol></nav></details></aside><pre><code>### Math Primers
</code></pre><p><a class="internal-link broken">maths_primer.pdf</a></p><p>#TODO page 6 of mi exercises perceptron</p><a href=#mi1><h3 id=mi1><span class=hanchor arialabel=Anchor># </span>MI1</h3></a><p><a href=/quartz/Notation-MI1 rel=noopener class=internal-link data-src=/quartz/Notation-MI1>Notation MI1</a></p><a href=#12-perceptron-and-feature-extraction><h2 id=12-perceptron-and-feature-extraction><span class=hanchor arialabel=Anchor># </span>1.2 Perceptron and feature extraction</h2></a><p>How to train a connectionnist neuron & function fitting?
Training a <a href=/quartz/Perceptron rel=noopener class=internal-link data-src=/quartz/Perceptron>Perceptron</a> allows for training of linear patterns.
For <a href=/quartz/Multi-Layer-Perceptron rel=noopener class=internal-link data-src=/quartz/Multi-Layer-Perceptron>Multi-Layer Perceptron</a> you need to introduce Non-Linearity with
<a class="internal-link broken">Transfer functions</a> you can also add do <strong>Feature extraction</strong> by using filters like a <a href=/quartz/Convolution rel=noopener class=internal-link data-src=/quartz/Convolution>Convolution</a>.</p><a href=#13-multi-layer-perceptron><h2 id=13-multi-layer-perceptron><span class=hanchor arialabel=Anchor># </span>1.3 Multi-Layer Perceptron</h2></a><a href=#model><h3 id=model><span class=hanchor arialabel=Anchor># </span>Model</h3></a><p>!<a href=/quartz/Multi-Layer-Perceptron rel=noopener class=internal-link data-src=/quartz/Multi-Layer-Perceptron>Multi-Layer Perceptron</a></p><a href=#performance-measure><h3 id=performance-measure><span class=hanchor arialabel=Anchor># </span>Performance Measure</h3></a><p><a href=/quartz/Cost-Function rel=noopener class=internal-link data-src=/quartz/Cost-Function>Cost Function</a> but we don&rsquo;t have perfect data distributions, so we need to use <a href=/quartz/Empirical-Risk-Minimisation rel=noopener class=internal-link data-src=/quartz/Empirical-Risk-Minimisation>Empirical Risk Minimisation</a> because of the unperfections of <a href=/quartz/Empirical-Risk-Minimisation rel=noopener class=internal-link data-src=/quartz/Empirical-Risk-Minimisation>ERM</a> there is a risk of overfitting and underfitting.</p><a href=#optimisation-of-model-parameters><h3 id=optimisation-of-model-parameters><span class=hanchor arialabel=Anchor># </span>Optimisation of model parameters</h3></a><p>We will use <a href=/quartz/Gradient-Descent rel=noopener class=internal-link data-src=/quartz/Gradient-Descent>Gradient Descent</a> to find the optimal parameters <a href=/quartz/Backpropagation rel=noopener class=internal-link data-src=/quartz/Backpropagation>Backpropagation</a>
12/07/2022 and</p><a href=#validation><h3 id=validation><span class=hanchor arialabel=Anchor># </span>Validation</h3></a><p><a href=/quartz/Cross-Validation rel=noopener class=internal-link data-src=/quartz/Cross-Validation>Cross Validation</a></p><a href=#14-additionnal-topics><h2 id=14-additionnal-topics><span class=hanchor arialabel=Anchor># </span>1.4 Additionnal topics</h2></a><a href=#stochastic-approximation-and-online-learning><h3 id=stochastic-approximation-and-online-learning><span class=hanchor arialabel=Anchor># </span>Stochastic Approximation and Online Learning</h3></a><p>When doing <a href=/quartz/Gradient-Descent rel=noopener class=internal-link data-src=/quartz/Gradient-Descent>Gradient Descent</a> instead of calculating the gradient over all the training samples ie <a href=/quartz/Batch-Learning rel=noopener class=internal-link data-src=/quartz/Batch-Learning>Batch-Learning</a> you can just calculate it over one training sample, and update the weights with that this is <a href=/quartz/Online-Learning rel=noopener class=internal-link data-src=/quartz/Online-Learning>Online Learning</a>.</p><p><a href=/quartz/Online-Learning rel=noopener class=internal-link data-src=/quartz/Online-Learning>Online learning</a> more exploration than <a href=/quartz/Mini-Batch rel=noopener class=internal-link data-src=/quartz/Mini-Batch>Mini-Batch</a> more exploration that <a href=/quartz/Batch-Learning rel=noopener class=internal-link data-src=/quartz/Batch-Learning>Batch-Learning</a>.</p><p><a href=/quartz/Online-Learning rel=noopener class=internal-link data-src=/quartz/Online-Learning>Online Learning</a></p><a href=#improving-gradient-descent-optimization-batch><h3 id=improving-gradient-descent-optimization-batch><span class=hanchor arialabel=Anchor># </span>Improving Gradient Descent Optimization (Batch)</h3></a><p>Gradient descente can be optimised with the following algorithm
Take the gradient $\Delta w$ and:</p><ul><li>do a <a class="internal-link broken">line search</a> for the lowest point in this line, this gives you step size.</li><li>Then take the <a href=/quartz/Conjugate-Gradient rel=noopener class=internal-link data-src=/quartz/Conjugate-Gradient>Conjugate Gradient</a> and continue in this direction.
You should reuse the gradient every $N$ steps, as <a href=/quartz/Conjugate-Gradient rel=noopener class=internal-link data-src=/quartz/Conjugate-Gradient>Conjugate Gradient</a>s become less and less acurate over time.</li></ul><a href=#the-conjugate-gradient-method><h3 id=the-conjugate-gradient-method><span class=hanchor arialabel=Anchor># </span>The Conjugate Gradient Method</h3></a><p>#tocomplete</p><a href=#overfitting-and-underfitting><h3 id=overfitting-and-underfitting><span class=hanchor arialabel=Anchor># </span>Overfitting and Underfitting</h3></a><p>$E_{Train}$ and $E_{Gen}$ relationship</p><a href=#bias-and-variance><h3 id=bias-and-variance><span class=hanchor arialabel=Anchor># </span>Bias and Variance</h3></a><p><a href=/quartz/Bias-and-variance-of-a-model rel=noopener class=internal-link data-src=/quartz/Bias-and-variance-of-a-model>Bias and variance of a model</a> helps us understand a model class better</p><a href=#regularization><h3 id=regularization><span class=hanchor arialabel=Anchor># </span>Regularization</h3></a><p>Sometimes we will se generalisation problems, to fight against those we can use <a href=/quartz/Regularization rel=noopener class=internal-link data-src=/quartz/Regularization>Regularization</a> to shape the learning surfaces in a certains way.
<a class="internal-link broken">Pasted image 20230215205317.png</a>
Here the Gradient descent will naturaly go optimise $w_{2}$ easier as it is steeper, and so the compromise with $E^R$ will be found in this direction. In the direction $w_{1}$ the descent is shallow so it will be difficult to learn for the gradient descent, and the $w_{1}$ might endup on large values which will give large variance/overfitting/wigly function.</p><p>RMQ test set and validation set are being swapped in the lecture
<a class="internal-link broken">Weight Decay</a> #tocomplete</p><a href=#classification-problems-multi-class><h3 id=classification-problems-multi-class><span class=hanchor arialabel=Anchor># </span>Classification Problems (Multi-Class)</h3></a><p>How to use the networks for classification ? There is no notion of distance between classes, so use probabilities.
As an output normalisation step use <a href=/quartz/Softmax rel=noopener class=internal-link data-src=/quartz/Softmax>Softmax</a> to turn the outputs into a probability distribution summing to 1.
So then we have to compare the true probabilty distribution and the estimated one :
Â <a href=/quartz/KullbackLeibler-divergence rel=noopener class=internal-link data-src=/quartz/KullbackLeibler-divergence>Kullbackâ€“Leibler divergence</a>
Â But we can remove a part of the equation independant of model parameters and then adjuste for the fact that we don&rsquo;t know the true distributions &mldr; and we will get the <a href=/quartz/Cross-Entropy rel=noopener class=internal-link data-src=/quartz/Cross-Entropy>Cross-Entropy</a>.
We can then do <a href=/quartz/Gradient-Descent rel=noopener class=internal-link data-src=/quartz/Gradient-Descent>Gradient Descent</a> with this loss.
But if we want to know the Cost as a performance measure,we need to make adjustments knowing the cost of a decision <a href=/quartz/Cross-Entropy-Loss rel=noopener class=internal-link data-src=/quartz/Cross-Entropy-Loss>Cross-Entropy Loss</a>, sometimes the cost is the same for all possibilities.</p><p>Deep <a href=/quartz/Multi-Layer-Perceptron rel=noopener class=internal-link data-src=/quartz/Multi-Layer-Perceptron>MLP</a>s where tried before, problem was computation power, too small datasets, and hard to train for ex <a href=/quartz/Vanishing-Gradient rel=noopener class=internal-link data-src=/quartz/Vanishing-Gradient>Vanishing Gradient</a>.
after the 2000&rsquo;s</p><a href=#15-deep-learning><h2 id=15-deep-learning><span class=hanchor arialabel=Anchor># </span>1.5 Deep Learning</h2></a><ul><li><a href=/quartz/Rectified-Linear-Unit rel=noopener class=internal-link data-src=/quartz/Rectified-Linear-Unit>Rectified Linear Unit</a> activation function instead of <a href=/quartz/Sigmoid rel=noopener class=internal-link data-src=/quartz/Sigmoid>Sigmoid</a> function solves <a href=/quartz/Vanishing-Gradient rel=noopener class=internal-link data-src=/quartz/Vanishing-Gradient>Vanishing Gradient</a> !</li><li>Intelligent initialization, transfer learning <a href=/quartz/Auto-Encoders rel=noopener class=internal-link data-src=/quartz/Auto-Encoders>Auto-Encoders</a> make them easier to train.</li><li>Regularisation Methods reduce overfitting</li><li>Problem adapted neural networks <a href=/quartz/Convolutionnal-Neural-Network rel=noopener class=internal-link data-src=/quartz/Convolutionnal-Neural-Network>CNN</a>, <a href=/quartz/Recurent-Neural-Network rel=noopener class=internal-link data-src=/quartz/Recurent-Neural-Network>RNN</a> have higher performance with less parameters.</li></ul><a href=#auto-encoders><h3 id=auto-encoders><span class=hanchor arialabel=Anchor># </span><a href=/quartz/Auto-Encoders rel=noopener class=internal-link data-src=/quartz/Auto-Encoders>Auto-Encoders</a></h3></a><p>Example <a href=/quartz/Deep-Belief-Network rel=noopener class=internal-link data-src=/quartz/Deep-Belief-Network>Deep belief network</a> Or <a class="internal-link broken">Sim CLR</a> to initialise part of the network in unsupervised way.</p><a href=#new-training-method><h3 id=new-training-method><span class=hanchor arialabel=Anchor># </span>New training method</h3></a><p><a href=/quartz/Mini-Batch rel=noopener class=internal-link data-src=/quartz/Mini-Batch>Mini-Batch</a> is a compromise between <a href=/quartz/Online-Learning rel=noopener class=internal-link data-src=/quartz/Online-Learning>Online Learning</a> and <a href=/quartz/Batch-Learning rel=noopener class=internal-link data-src=/quartz/Batch-Learning>Batch-Learning</a> who&rsquo;s main advantage is to be efficient to compute with GPU.</p><a href=#regularization-1><h3 id=regularization-1><span class=hanchor arialabel=Anchor># </span>Regularization</h3></a><p><a class="internal-link broken">Weight Decay</a> or <a class="internal-link broken">Data Augmentation</a>, <a href=/quartz/Dropout rel=noopener class=internal-link data-src=/quartz/Dropout>Dropout</a></p><p>We use <a href=/quartz/Tensor rel=noopener class=internal-link data-src=/quartz/Tensor>Tensor</a>s</p><a href=#convolutionnal-neural-network><h3 id=convolutionnal-neural-network><span class=hanchor arialabel=Anchor># </span><a href=/quartz/Convolutionnal-Neural-Network rel=noopener class=internal-link data-src=/quartz/Convolutionnal-Neural-Network>Convolutionnal Neural Network</a></h3></a><p><a href=/quartz/Convolution rel=noopener class=internal-link data-src=/quartz/Convolution>Convolution</a>
We use mulitple convolutionnal filters combined to extract multiple filters, but that increases the input space of the next layer a lot, so we need to reduce this.
<a href=/quartz/Max-Pooling rel=noopener class=internal-link data-src=/quartz/Max-Pooling>Max Pooling</a> can introduce non-linearty do <a href=/quartz/Downsampling rel=noopener class=internal-link data-src=/quartz/Downsampling>Downsampling</a> by using a stride. This is also called <a class="internal-link broken">feature pooling</a> ? which introduces invariances. <a href=/quartz/Max-Pooling rel=noopener class=internal-link data-src=/quartz/Max-Pooling>Max Pooling</a> can also be done across filters.</p><p>Interesting the 2nd, Convolution have less parameters
<a class="internal-link broken">Pasted image 20230216215630.png</a></p><p><a href=/quartz/Deconvolutional-Network rel=noopener class=internal-link data-src=/quartz/Deconvolutional-Network>Deconvolutional Network</a> is for segmentation for example, looks like an autoencoder. The output tensor can be the same size or bigger tham the input tensor.
When compression to an intermediary small representation, you have to do unpooling to reverse the effects of the <a class="internal-link broken">Pooling</a>.
<a class="internal-link broken">Unpooling</a> <a class="internal-link broken">Deconvolution</a></p><p>Pays offs Implicit feature learning
Same Data Different tasks</p><ul><li>Challenges Computationnaly</li><li>Prone to overfitiing -> Solution DataAugementation <a href=/quartz/Convolution rel=noopener class=internal-link data-src=/quartz/Convolution>Convolution</a>, <a href=/quartz/Dropout rel=noopener class=internal-link data-src=/quartz/Dropout>Dropout</a></li><li>adversial
Robins monroe condition</li></ul><p>After having handled images with a custon architecture let&rsquo;s go tackle <a href=/quartz/Time-Series rel=noopener class=internal-link data-src=/quartz/Time-Series>Time Series</a>.
In the Times series tensors past events can influence the future but the future can&rsquo;t influence the past. But you can break the causility when learning.</p><a href=#161><h2 id=161><span class=hanchor arialabel=Anchor># </span>1.6.1</h2></a><p>Use <a href=/quartz/Convolutionnal-Neural-Network rel=noopener class=internal-link data-src=/quartz/Convolutionnal-Neural-Network>CNN</a> on the a time interval, you can then also implement time dilation.
How to get rid of time window ?</p><a href=#162-recurent-neural-networks><h2 id=162-recurent-neural-networks><span class=hanchor arialabel=Anchor># </span>1.6.2 Recurent Neural Networks</h2></a><p>#tocomplete
<a href=/quartz/Recurent-Neural-Network rel=noopener class=internal-link data-src=/quartz/Recurent-Neural-Network>Recurent Neural Network</a></p><p>How to train <a href=/quartz/Recurent-Neural-Network rel=noopener class=internal-link data-src=/quartz/Recurent-Neural-Network>RNN</a> ?? With normal Backprop $O(H^2)$Fixed with <a href=/quartz/Backpropagation-through-time rel=noopener class=internal-link data-src=/quartz/Backpropagation-through-time>Backpropagation through time</a>. #tocomplete</p><a href=#long-term-dependencie><h2 id=long-term-dependencie><span class=hanchor arialabel=Anchor># </span>Long-term dependencie</h2></a><p><a href=/quartz/Echo-state-Network rel=noopener class=internal-link data-src=/quartz/Echo-state-Network>Echo-state Network</a>
#tocomplete
In order to have longer memory spans we use Memory units as in
<a href=/quartz/LSTM rel=noopener class=internal-link data-src=/quartz/LSTM>LSTM</a>, they are state of the art</p><a href=#other-recurent-architectures><h3 id=other-recurent-architectures><span class=hanchor arialabel=Anchor># </span>Other recurent architectures</h3></a><a href=#17-rbf-networks><h2 id=17-rbf-networks><span class=hanchor arialabel=Anchor># </span>1.7 RBF networks</h2></a><p><a href=/quartz/K-Nearest-Neighboor rel=noopener class=internal-link data-src=/quartz/K-Nearest-Neighboor>K-Nearest Neighboor</a>
<a href=/quartz/Parzen-Window-Classification rel=noopener class=internal-link data-src=/quartz/Parzen-Window-Classification>Parzen Window Classification</a>
<a href=/quartz/Radial-Basis-Function-network rel=noopener class=internal-link data-src=/quartz/Radial-Basis-Function-network>RBF Network</a> is a compromise betweent the two methods, we use <strong>k</strong> representative functions (not points).</p><a href=#21-statistical-learning-theory><h1 id=21-statistical-learning-theory><span class=hanchor arialabel=Anchor># </span>2.1 Statistical learning theory</h1></a><a href=#for-linear-classification><h3 id=for-linear-classification><span class=hanchor arialabel=Anchor># </span>For linear classification</h3></a><p>Their is a key number of points $p$ over the number of dimensions $N$
where generalisation happens.
For linear classifier $\frac{p}{N}>2$</p><a href=#formulation-of-the-inductive-learning-problem><h3 id=formulation-of-the-inductive-learning-problem><span class=hanchor arialabel=Anchor># </span>Formulation of the inductive learning problem</h3></a><p>Three questions :</p><ul><li>When does inductive learning through <a href=/quartz/Empirical-Risk-Minimisation rel=noopener class=internal-link data-src=/quartz/Empirical-Risk-Minimisation>ERM</a> work ?</li><li>How strongly do $E_{w_{0}}$ differe from $E_{w_{p}}$ on a finit amount of samples.</li><li>Does $E^R$ reflect $E$. ?
Answers :
<a href=/quartz/VC-dimensions rel=noopener class=internal-link data-src=/quartz/VC-dimensions>VC dimensions</a>
small $d_{VC}$ dimension means small sample size sufficient for learning
large $d_{VC}$ large samples size needed
<a href=/quartz/Empirical-Risk-Minimisation rel=noopener class=internal-link data-src=/quartz/Empirical-Risk-Minimisation>ERM</a> $\neq$ <a href=/quartz/Structural-Risk-Minimisation rel=noopener class=internal-link data-src=/quartz/Structural-Risk-Minimisation>SRM</a></li></ul><a href=#22-support-vector-machines><h2 id=22-support-vector-machines><span class=hanchor arialabel=Anchor># </span>2.2 Support vector machines</h2></a><p><a href=/quartz/Suport-Vector-Machines rel=noopener class=internal-link data-src=/quartz/Suport-Vector-Machines>Suport Vector Machines</a> assume zero training error, this is impractical so in practive we are going to give a bit of slack with <a href=/quartz/C-Support-Vector-Machines rel=noopener class=internal-link data-src=/quartz/C-Support-Vector-Machines>C-SVM</a> .
<a href=/quartz/Suport-Vector-Machines rel=noopener class=internal-link data-src=/quartz/Suport-Vector-Machines>SVM</a> also need to work in higher dimensional space so <a href=/quartz/Kernel-Trick rel=noopener class=internal-link data-src=/quartz/Kernel-Trick>Kernel Trick</a>
Kernel has to be positive semi difinite</p><p>Jumped derivation : Construct of inner product space
<a href=/quartz/C-Support-Vector-Machines rel=noopener class=internal-link data-src=/quartz/C-Support-Vector-Machines>C-SVM</a> introduces slack variable $@\phi\alpha$
You solve the dual problem with Sequential Minimal Optimization</p><a href=#support-vector-regression><h3 id=support-vector-regression><span class=hanchor arialabel=Anchor># </span>Support vector regression</h3></a><a href=#31-uncertainty-and-inference><h2 id=31-uncertainty-and-inference><span class=hanchor arialabel=Anchor># </span>3.1 Uncertainty and inference</h2></a><p>normalisation ??
Marginialisation ?? #tocomplete</p><p>We have a knowledge base of rules written in conditional probabilities.
There is the concept of <a href=/quartz/Conditionnal-Independance rel=noopener class=internal-link data-src=/quartz/Conditionnal-Independance>Conditionnal Independance</a>
We can do dependency graphs.
The <a href=/quartz/Conditionnal-Independance rel=noopener class=internal-link data-src=/quartz/Conditionnal-Independance>Conditionnal Independance</a> helps us to reduce computation complexity by removing from the database the cases of combination of events that are irrelevant to store knowing their independance.</p><a href=#32-bayesian-networks><h2 id=32-bayesian-networks><span class=hanchor arialabel=Anchor># </span>3.2 Bayesian networks</h2></a><p>#tocomplete see exact calculations
<a href=/quartz/Markov-Blanket rel=noopener class=internal-link data-src=/quartz/Markov-Blanket>Markov Blanket</a> is used to find <a href=/quartz/Conditionnal-Independance rel=noopener class=internal-link data-src=/quartz/Conditionnal-Independance>Conditionnal Independance</a> in a <a class="internal-link broken">Directed Acyclic Graph</a>. if point node $X$ is not in the <a href=/quartz/Markov-Blanket rel=noopener class=internal-link data-src=/quartz/Markov-Blanket>Markov Blanket</a> of $A$ then $X$ is conditionnaly indpendant from $A$.
<a href=/quartz/Moral-graph rel=noopener class=internal-link data-src=/quartz/Moral-graph>Moral graph</a></p><a href=#graph-inference><h2 id=graph-inference><span class=hanchor arialabel=Anchor># </span>Graph inference</h2></a><p>We need <a href=/quartz/Bipartite-Graph rel=noopener class=internal-link data-src=/quartz/Bipartite-Graph>Bipartite Graph</a> <a href=/quartz/Tree rel=noopener class=internal-link data-src=/quartz/Tree>Tree</a>s graphs for efficient inference, they allow use to reduce the complexity of exonential in the number of variables to linear.
We then use the <a class="internal-link broken">sum product algorithm</a> combined with a message passing algorithm.</p><p>Problem when the conditional probabilty graphs that are not trees, we transform them into <a href=/quartz/Bipartite-Graph rel=noopener class=internal-link data-src=/quartz/Bipartite-Graph>Bipartite Graph</a> <a href=/quartz/Tree rel=noopener class=internal-link data-src=/quartz/Tree>Tree</a>s, by making them into <a href=/quartz/Junction-tree rel=noopener class=internal-link data-src=/quartz/Junction-tree>Junction tree</a>s using the concept of <a href=/quartz/cliques rel=noopener class=internal-link data-src=/quartz/cliques>cliques</a>.
<a class="internal-link broken">Separators</a> <a class="internal-link broken">Decomposable Graph</a> <a href=/quartz/Complete-graph rel=noopener class=internal-link data-src=/quartz/Complete-graph>Complete graph</a></p><p>In practice
transform the bayesian network to junction tree for efficient inference
do the inferance with the message passing algorithme</p><p><a class="internal-link broken">Pasted image 20230220185755.png</a></p><p>Proper decomposition
decomposable graph is a requirement for the building of Junction Trees
You then make the DAG-> <a href=/quartz/Moral-graph rel=noopener class=internal-link data-src=/quartz/Moral-graph>Moral graph</a> -><a href=/quartz/Chordal-Decomposable-graph rel=noopener class=internal-link data-src=/quartz/Chordal-Decomposable-graph>Chordal Decomposable graph</a> -> <a href=/quartz/Bipartite-Graph rel=noopener class=internal-link data-src=/quartz/Bipartite-Graph>Bipartite Graph</a> (identify <a href=/quartz/cliques rel=noopener class=internal-link data-src=/quartz/cliques>cliques</a> and <a class="internal-link broken">Separators</a> also using reverse topological sorting)-> <a href=/quartz/Junction-tree rel=noopener class=internal-link data-src=/quartz/Junction-tree>Junction tree</a>
cliques are <a href=/quartz/Maximaly-complete-Graph rel=noopener class=internal-link data-src=/quartz/Maximaly-complete-Graph>Maximaly complete Graph</a>.</p><p>To then use the <a class="internal-link broken">Message Passing</a> algorithm</p><p>Inference</p><a href=#33-bayesian-inference-and-neural-networks><h2 id=33-bayesian-inference-and-neural-networks><span class=hanchor arialabel=Anchor># </span>3.3 Bayesian inference and neural networks</h2></a><a href=#reinforcment-learning><h1 id=reinforcment-learning><span class=hanchor arialabel=Anchor># </span>Reinforcment Learning</h1></a><a href=#41-valuation><h2 id=41-valuation><span class=hanchor arialabel=Anchor># </span>4.1 Valuation</h2></a><p>Transition model (matrix) : Says what you can do in this world but with probabilities, so one action starting from the same state doesn&rsquo;t always yield the same result.</p><p>Policy : What you are going to do</p><p>return(cumulative reward) : discount factor $\sum\limits_{ t\to inf }\gamma r^t$</p><p>Value function : Expected return starting from this state with this policy
Measures quality of policy $\pi$</p><p>We can also calculate value functions for all possible states</p><p>Optimal policy : Best policy possible
Optimal value function : Value of a state when following the optimal policy</p><p>Optimal policy knowing optimal value function $V^*$ <a class="internal-link broken">Pasted image 20230221194911.png</a>
Doesn&rsquo;t scale well for large state spaces</p><a href=#42-improvment><h2 id=42-improvment><span class=hanchor arialabel=Anchor># </span>4.2 Improvment</h2></a><p><a href=/quartz/Machine-Intelligence-Abstract rel=noopener class=internal-link data-src=/quartz/Machine-Intelligence-Abstract>Machine Intelligence Abstract</a></p><p><a href=/quartz/tutorial_02_fitting.notes.pdf rel=noopener class=internal-link data-src=/quartz/tutorial_02_fitting.notes.pdf>tutorial_02_fitting.notes.pdf</a></p><p><a href=/quartz/Function-Fitting rel=noopener class=internal-link data-src=/quartz/Function-Fitting>Function Fitting</a></p><ol><li><a href=/quartz/Cost-Function rel=noopener class=internal-link data-src=/quartz/Cost-Function>Cost Function</a></li><li><a href=/quartz/Model-Performance-measure rel=noopener class=internal-link data-src=/quartz/Model-Performance-measure>Model Performance measure</a></li><li><a class="internal-link broken">Model Selection</a></li><li><a href=/quartz/Gradient-learning rel=noopener class=internal-link data-src=/quartz/Gradient-learning>Gradient learning</a></li></ol><p>16/11</p><a href=#deep-learning><h2 id=deep-learning><span class=hanchor arialabel=Anchor># </span>Deep Learning</h2></a><p>no course on the 15/12 thurday instead on wednesday</p><p>RELU solves the vanishing gradient problem</p><p>Why cant we have a the anlytical solution ?</p><p>if we look at the equation
We have to fit all the data on one equation
analytical</p><ul><li>no closed
<a href=/quartz/Regularization rel=noopener class=internal-link data-src=/quartz/Regularization>Regularization</a> is the solution to a model that is getting to complex to fit the data, and that therefore won&rsquo;t generalise well</li></ul><p>#incomplet complet with the courses slides
scope
batch
mini-batch
online</p><p>impulse term
adaptive step size</p><p>line search helps to decide the learning step, using parabolic interpolation, takes the minima of the parabola using a few points we have.
etc &mldr; with each time new points to be;ore accurate by looking at the loss.
Line search can find local minima !</p><p>limitations always perpendicular</p><p>Solution</p><a href=#conjugate-gradient-method><h2 id=conjugate-gradient-method><span class=hanchor arialabel=Anchor># </span>Conjugate gradient method</h2></a><p>Example of questions :
For multi class classification what loss do we use ?
Whzt transfer function on the last ?
Smt like that</p><p>24/11/2022</p><a href=#regularisation><h2 id=regularisation><span class=hanchor arialabel=Anchor># </span>Regularisation</h2></a><p><a href=/quartz/Regularization rel=noopener class=internal-link data-src=/quartz/Regularization>Regularization</a></p><a href=#validation-1><h2 id=validation-1><span class=hanchor arialabel=Anchor># </span>Validation</h2></a><p>Nested cross validation pretty cool</p><p>Preprocessing interesting #ToReview
Use same preprocessing for every</p><p>08/12</p><p><a href=/quartz/Recurent-Neural-Network rel=noopener class=internal-link data-src=/quartz/Recurent-Neural-Network>Recurent Neural Network</a>
<a href=/quartz/Echo-state-Network rel=noopener class=internal-link data-src=/quartz/Echo-state-Network>Echo-state Network</a></p><a href=#lstm><h2 id=lstm><span class=hanchor arialabel=Anchor># </span><a href=/quartz/LSTM rel=noopener class=internal-link data-src=/quartz/LSTM>LSTM</a></h2></a><p>exercice
compare train and holdout performance
Simple model is underfitting</p><p>14/12
08 Class Imbalance and RBF-Networks</p><a href=#class-imbalance><h2 id=class-imbalance><span class=hanchor arialabel=Anchor># </span>Class imbalance:</h2></a><p>If we have the expected label true 95% of the time, then if model -> true, then it has accurracy of 95%.
Counteract <a href=/quartz/Weighted-error rel=noopener class=internal-link data-src=/quartz/Weighted-error>Weighted error</a>
<a href=/quartz/Accurracy rel=noopener class=internal-link data-src=/quartz/Accurracy>Accurracy</a></p><p><a class="internal-link broken">Machine intelligence 1 2022-12-14 13.43.30.excalidraw</a></p><a href=#k-nearest-neighbours><h2 id=k-nearest-neighbours><span class=hanchor arialabel=Anchor># </span><a href=/quartz/K-Nearest-Neighbours rel=noopener class=internal-link data-src=/quartz/K-Nearest-Neighbours>K Nearest Neighbours</a></h2></a><a href=#radial-basis-function><h2 id=radial-basis-function><span class=hanchor arialabel=Anchor># </span><a href=/quartz/Radial-Basis-Function rel=noopener class=internal-link data-src=/quartz/Radial-Basis-Function>Radial Basis Function</a></h2></a><p><a href=/quartz/Radial-Basis-Function-network rel=noopener class=internal-link data-src=/quartz/Radial-Basis-Function-network>Radial Basis Function network</a></p><a href=#things-to-learn><h1 id=things-to-learn><span class=hanchor arialabel=Anchor># </span>Things to learn</h1></a><p>SVM</p><ul><li><p><input disabled type=checkbox> Primal problem dual problem def</p></li><li><p><input disabled type=checkbox> Fin primal var and dual var</p></li><li><p><input disabled type=checkbox> Derive lagrangian</p></li><li><p><input disabled type=checkbox> sparse variables</p></li><li><p><input checked disabled type=checkbox> C-SVM</p></li><li><p><input checked disabled type=checkbox> Chaine Rule</p></li><li><p><input checked disabled type=checkbox> Backpropagation</p></li><li><p><input checked disabled type=checkbox> k nearest neighboor</p></li><li><p><input checked disabled type=checkbox> back Propagation for CNN let it go</p></li><li><p><input disabled type=checkbox> Robins monroe condition</p></li><li><p><input checked disabled type=checkbox> See RNN exact formulas</p></li><li><p><input disabled type=checkbox> Bellmann equation</p></li><li><p><input disabled type=checkbox> Derive the bellman equation from &mldr; see exam</p></li><li><p><input disabled type=checkbox> Bayesian networks</p></li><li><p><input disabled type=checkbox> finish the moke exam</p></li><li><p><input disabled type=checkbox> derive dual from primal</p></li><li><p><input disabled type=checkbox> Do the graph things</p></li><li><p><input disabled type=checkbox> the message passing</p></li><li><p><input disabled type=checkbox> Inference for Bayesian network</p></li><li><p><input disabled type=checkbox> 2. (6 points) Derive the Bellman equation from the value function V</p></li><li><p><input disabled type=checkbox> Learn to calculate the C of Statistical learning</p></li><li><p><input disabled type=checkbox> Message passing, what do they mean with marginalisation</p></li><li><p><input disabled type=checkbox> CSVM</p></li><li><p><input disabled type=checkbox> RL</p></li><li><p><input disabled type=checkbox> Message passing alg</p></li><li><p><input disabled type=checkbox></p></li></ul><a href=#questions><h2 id=questions><span class=hanchor arialabel=Anchor># </span>Questions</h2></a><p>is $\sigma_{i}$ different for every function yeah probably, but in practive isn&rsquo;t really</p><p>CSVM can be assymetric SVM not</p><p>Support vector regression
the points on the red line are not support vectors</p><p>The margin is where $y_{t}(xw+b)\geq 1$ is met</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/quartz/Tu-Courses/ data-ctx="Machine intelligence 1" data-src=/Tu-Courses class=internal-link>Tu Courses</a></li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://theo-michel.github.io/quartz/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Jacky Zhao using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2023</p><ul><li><a href=https://theo-michel.github.io/quartz/>Home</a></li><li><a href=https://twitter.com/_jzhao>Twitter</a></li><li><a href=https://github.com/jackyzha0>GitHub</a></li></ul></footer></div></div></body></html>